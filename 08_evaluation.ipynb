{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">182</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m182\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Toward Co-creative Dungeon Generation via Transfer Learning\n",
      "\n",
      "Summary: Co-creative Procedural Content Generation via Machine Learning (PCGML) refers to systems where a PCGML agent and a human work together to produce output content. One of the limitations of co-creative PCGML is that it requires co-creative training data for a PCGML agent to learn to interact with humans. However, acquiring this data is a difficult and time-consuming process. In this work, we propose approximating human-AI interaction data and employing transfer learning to adapt learned co-creative knowledge from one game to a different game. We explore this approach for co-creative Zelda dungeon room generation.\n",
      "\n",
      "Page Body: training data, since their system produces entire complete levels\n",
      "without human interaction, where we require data from iterative,\n",
      "turn-based interactions. Thus, we have no choice but to approxi-\n",
      "mate the required interaction data from the non-interactive VGLC\n",
      "data [19].\n",
      "Essentially, we look to approximate a hypothetical situation in\n",
      "which the original human level designers of Zelda designed the\n",
      "rooms with an AI agent partner. However, there is no literature\n",
      "on how best to approximate interaction data from non-interaction\n",
      "data. Thus, we employ three different strategies. In all cases, we\n",
      "take a final complete room and iteratively remove groups of tiles.\n",
      "This gives a sequence of states with an initial complete room and a\n",
      "final, empty or nearly empty room (depending on the strategy). We\n",
      "can then reverse this sequence to approximate the kind of iterative,\n",
      "incremental design process we require. The sequence of removals\n",
      "becomes a sequence of additions, a series of actions we treat as\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Re-defined successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# 1. Define the Helper Functions (if not already defined)\n",
    "def retrieve_docs(query, k=4):\n",
    "    \"\"\"Actual retrieval from FAISS\"\"\"\n",
    "    return docstore.similarity_search(query, k=k)\n",
    "\n",
    "def docs2str(docs):\n",
    "    \"\"\"Format documents for the prompt\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"Extract string from the chain output\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if hasattr(token, 'content'):\n",
    "            yield token.content\n",
    "        elif isinstance(token, dict) and 'output' in token:\n",
    "            yield token['output']\n",
    "        elif isinstance(token, str):\n",
    "            yield token\n",
    "\n",
    "# 2. Define the Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "# 3. Build the Chain (CORRECTED VERSION)\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "# ERROR WAS HERE: We must actually call retrieve_docs, not just pass 'x'\n",
    "context_getter = (\n",
    "    RunnableLambda(lambda d: retrieve_docs(d['input'], k=4))\n",
    "    | long_reorder\n",
    "    | RunnableLambda(docs2str)\n",
    ")\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input': (lambda x: x)}\n",
    "    | RunnableAssign({'context': context_getter})\n",
    ")\n",
    "\n",
    "# Generator Chain\n",
    "# We use 'instruct_llm' (Llama 3.1 70B) because it follows instructions better than 8B\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "generator_chain = chat_prompt | llm\n",
    "\n",
    "# Final RAG Chain\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "print(\"RAG Chain Re-defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the challenge of acquiring co-creative training data for PCGML agents impact the development of </span>\n",
       "<span style=\"font-weight: bold\">co-creative procedural content generation systems?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the challenge of acquiring co-creative training data for PCGML agents impact the development of \u001b[0m\n",
       "\u001b[1mco-creative procedural content generation systems?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Acquiring co-creative training data for PCGML agents is a difficult and time-consuming process, which is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">one of the limitations of co-creative PCGML.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Acquiring co-creative training data for PCGML agents is a difficult and time-consuming process, which is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mone of the limitations of co-creative PCGML.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can the use of ensembles of Markov chains, as investigated in the context of Mega Man level generation, </span>\n",
       "<span style=\"font-weight: bold\">be applied to improve the co-creative procedural content generation process, where a human and a machine learning </span>\n",
       "<span style=\"font-weight: bold\">agent work together to produce output content?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can the use of ensembles of Markov chains, as investigated in the context of Mega Man level generation, \u001b[0m\n",
       "\u001b[1mbe applied to improve the co-creative procedural content generation process, where a human and a machine learning \u001b[0m\n",
       "\u001b[1magent work together to produce output content?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: While the documents do not directly address this question, they do provide insights that suggest a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">potential connection. The use of ensembles of Markov chains in Mega Man level generation (Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) may help to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capture the true variance present in underlying data, which could be beneficial in a co-creative setting where </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">human interaction data is scarce. Additionally, the idea of transfer learning proposed in Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, which </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">involves adapting learned co-creative knowledge from one game to a different game, could potentially be applied to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">leverage the benefits of ensemble methods in a co-creative context.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: While the documents do not directly address this question, they do provide insights that suggest a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpotential connection. The use of ensembles of Markov chains in Mega Man level generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m may help to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapture the true variance present in underlying data, which could be beneficial in a co-creative setting where \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhuman interaction data is scarce. Additionally, the idea of transfer learning proposed in Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m, which \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minvolves adapting learned co-creative knowledge from one game to a different game, could potentially be applied to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mleverage the benefits of ensemble methods in a co-creative context.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do the approaches to procedural content generation in the Mega Man and Zelda games differ in terms of</span>\n",
       "<span style=\"font-weight: bold\">their representation of game state and action spaces?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How do the approaches to procedural content generation in the Mega Man and Zelda games differ in terms of\u001b[0m\n",
       "\u001b[1mtheir representation of game state and action spaces?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The Mega Man approach, as described in the first paper, focuses on a room-based representation, where each </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">room is assigned a coordinate according to its type and the locations of adjacent rooms. In contrast, the Zelda </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach, as described in the second paper, employs a tile-based representation, where each room is represented as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an 11x16 matrix with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> possible tile types, each representing a category of Zelda dungeon entities. This suggests </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the Mega Man approach is more focused on the spatial arrangement of rooms, while the Zelda approach is more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">focused on the composition of individual rooms.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The Mega Man approach, as described in the first paper, focuses on a room-based representation, where each \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mroom is assigned a coordinate according to its type and the locations of adjacent rooms. In contrast, the Zelda \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproach, as described in the second paper, employs a tile-based representation, where each room is represented as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0man 11x16 matrix with \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m possible tile types, each representing a category of Zelda dungeon entities. This suggests \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat the Mega Man approach is more focused on the spatial arrangement of rooms, while the Zelda approach is more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfocused on the composition of individual rooms.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Original Question: How does the challenge of acquiring co-creative training data for PCGML agents impact the </span>\n",
       "<span style=\"font-weight: bold\">development of co-creative procedural content generation systems?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mOriginal Question: How does the challenge of acquiring co-creative training data for PCGML agents impact the \u001b[0m\n",
       "\u001b[1mdevelopment of co-creative procedural content generation systems?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a great question. The challenge of acquiring co-creative training data for PCGML agents is a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significant limitation in the development of co-creative procedural content generation systems. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">According to Zisen Zhou and Matthew Guzdial's work on </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Toward Co-creative Dungeon Generation via Transfer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Learning\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, acquiring co-creative training data is a difficult and time-consuming process. This is because it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">requires human-AI interaction data, which can be hard to obtain, especially for new games.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, the authors mention that running user subject studies for every game would be costly and difficult to find</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a user base with relevant design experience for every game. This makes it challenging to develop high-quality </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">co-creative agents without requiring game-specific user studies.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To address this challenge, Zhou and Guzdial propose approximating human-AI interaction data and employing transfer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">learning to adapt learned co-creative knowledge from one game to a different game. This approach has the potential </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to reduce the need for game-specific user studies and make it easier to develop high-quality co-creative agents.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the challenge of acquiring co-creative training data is a significant hurdle in the development of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">co-creative procedural content generation systems, but researchers like Zhou and Guzdial are exploring innovative </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">solutions to overcome this challenge.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: That's a great question. The challenge of acquiring co-creative training data for PCGML agents is a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificant limitation in the development of co-creative procedural content generation systems. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAccording to Zisen Zhou and Matthew Guzdial's work on \u001b[0m\u001b[32m\"Toward Co-creative Dungeon Generation via Transfer \u001b[0m\n",
       "\u001b[32mLearning\"\u001b[0m\u001b[1;38;2;118;185;0m, acquiring co-creative training data is a difficult and time-consuming process. This is because it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrequires human-AI interaction data, which can be hard to obtain, especially for new games.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn fact, the authors mention that running user subject studies for every game would be costly and difficult to find\u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma user base with relevant design experience for every game. This makes it challenging to develop high-quality \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mco-creative agents without requiring game-specific user studies.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo address this challenge, Zhou and Guzdial propose approximating human-AI interaction data and employing transfer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlearning to adapt learned co-creative knowledge from one game to a different game. This approach has the potential \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto reduce the need for game-specific user studies and make it easier to develop high-quality co-creative agents.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the challenge of acquiring co-creative training data is a significant hurdle in the development of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mco-creative procedural content generation systems, but researchers like Zhou and Guzdial are exploring innovative \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msolutions to overcome this challenge.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Original Question: Can the use of ensembles of Markov chains, as investigated in the context of Mega Man level </span>\n",
       "<span style=\"font-weight: bold\">generation, be applied to improve the co-creative procedural content generation process, where a human and a </span>\n",
       "<span style=\"font-weight: bold\">machine learning agent work together to produce output content?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mOriginal Question: Can the use of ensembles of Markov chains, as investigated in the context of Mega Man level \u001b[0m\n",
       "\u001b[1mgeneration, be applied to improve the co-creative procedural content generation process, where a human and a \u001b[0m\n",
       "\u001b[1mmachine learning agent work together to produce output content?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the document I retrieved, it seems that the use of ensembles of Markov chains could indeed be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">applied to improve the co-creative procedural content generation process. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The authors of the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Ensemble Learning For Mega Man Level Generation\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> investigated the use of ensembles of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Markov chains for procedurally generating Mega Man levels and found that it improved measures of playability and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">stylistic similarity compared to a non-ensemble, existing Markov chain approach.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In the context of co-creative procedural content generation, where a human and a machine learning agent work </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">together to produce output content, using ensembles of Markov chains could potentially allow the machine learning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">agent to learn from the human's input and adapt to their creative style, while also generating content that is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">engaging and playable.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The authors' approach of dividing game levels into rooms and categorizing them using the concept of game paths </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">could be particularly useful in co-creative settings, as it allows the machine learning agent to understand the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">high-level structure of the level and generate room sequences that are coherent and engaging.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, it seems that the use of ensembles of Markov chains could be a promising approach for improving </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">co-creative procedural content generation, and I'd love to see more research in this area.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the document I retrieved, it seems that the use of ensembles of Markov chains could indeed be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapplied to improve the co-creative procedural content generation process. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe authors of the paper \u001b[0m\u001b[32m\"Ensemble Learning For Mega Man Level Generation\"\u001b[0m\u001b[1;38;2;118;185;0m investigated the use of ensembles of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMarkov chains for procedurally generating Mega Man levels and found that it improved measures of playability and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstylistic similarity compared to a non-ensemble, existing Markov chain approach.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn the context of co-creative procedural content generation, where a human and a machine learning agent work \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtogether to produce output content, using ensembles of Markov chains could potentially allow the machine learning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0magent to learn from the human's input and adapt to their creative style, while also generating content that is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mengaging and playable.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe authors' approach of dividing game levels into rooms and categorizing them using the concept of game paths \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcould be particularly useful in co-creative settings, as it allows the machine learning agent to understand the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhigh-level structure of the level and generate room sequences that are coherent and engaging.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, it seems that the use of ensembles of Markov chains could be a promising approach for improving \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mco-creative procedural content generation, and I'd love to see more research in this area.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Original Question: How do the approaches to procedural content generation in the Mega Man and Zelda games differ in</span>\n",
       "<span style=\"font-weight: bold\">terms of their representation of game state and action spaces?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mOriginal Question: How do the approaches to procedural content generation in the Mega Man and Zelda games differ in\u001b[0m\n",
       "\u001b[1mterms of their representation of game state and action spaces?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: To answer your question, let's take a look at the document I have on procedural content generation in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mega Man and Zelda games.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">From what I can see, there isn't a direct comparison between the approaches to procedural content generation in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mega Man and Zelda games in the provided document. However, we can look at the differences in representation of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">game state and action spaces between the two approaches used in Mega Man.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In the Mega Man game, Li et al. employed an ensemble of different Markov chains based on the concept of game paths </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to better capture both horizontal and vertical patterns (Li et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). They used a </span><span style=\"color: #008000; text-decoration-color: #008000\">\"game path\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> representation, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which is distinct from the player path, to describe the way a level itself moves along the horizontal and vertical </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">axes.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, there isn't any information provided on procedural content generation in Zelda games. The </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">document only talks about the Mega Man game and a comparison with Super Mario Bros.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that the document does mention that Sarkar et al. have modeled Mega Man levels along with levels </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from a large number of other games, including Zelda games, with Variational Autoencoders (VAEs) for the purpose of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recombining this content to create entirely new types of content (Sarkar et al., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). However, this is not a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">direct comparison between the approaches to procedural content generation in Mega Man and Zelda games.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Li et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Ensemble Learning For Mega Man Level Generation. FDG</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, August </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"></span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, Montreal, QC, Canada.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sarkar et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Modeling and Generating Levels of Video Games using Variational Autoencoders.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: To answer your question, let's take a look at the document I have on procedural content generation in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMega Man and Zelda games.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFrom what I can see, there isn't a direct comparison between the approaches to procedural content generation in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMega Man and Zelda games in the provided document. However, we can look at the differences in representation of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgame state and action spaces between the two approaches used in Mega Man.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn the Mega Man game, Li et al. employed an ensemble of different Markov chains based on the concept of game paths \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto better capture both horizontal and vertical patterns \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLi et al., \u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. They used a \u001b[0m\u001b[32m\"game path\"\u001b[0m\u001b[1;38;2;118;185;0m representation, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhich is distinct from the player path, to describe the way a level itself moves along the horizontal and vertical \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maxes.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOn the other hand, there isn't any information provided on procedural content generation in Zelda games. The \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdocument only talks about the Mega Man game and a comparison with Super Mario Bros.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that the document does mention that Sarkar et al. have modeled Mega Man levels along with levels \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfrom a large number of other games, including Zelda games, with Variational Autoencoders \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mVAEs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for the purpose of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrecombining this content to create entirely new types of content \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSarkar et al., \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. However, this is not a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdirect comparison between the approaches to procedural content generation in Mega Man and Zelda games.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLi et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Ensemble Learning For Mega Man Level Generation. FDG\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;38;2;118;185;0m, August \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;38;2;118;185;0m, Montreal, QC, Canada.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mSarkar et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Modeling and Generating Levels of Video Games using Variational Autoencoders.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    # We strip the \"Question: \" prefix if it was included in the string\n",
    "    query = q.replace(\"Question: \", \"\").strip()\n",
    "    \n",
    "    # Invoking your RAG pipeline\n",
    "    # Note: Ensure your rag_chain is configured to take the question as input\n",
    "    rag_answer_obj = rag_chain.invoke(query)\n",
    "    \n",
    "    # Depending on your chain, you might need rag_answer_obj.content or just rag_answer_obj\n",
    "    rag_answer = getattr(rag_answer_obj, 'content', rag_answer_obj)\n",
    "    \n",
    "    rag_answers += [rag_answer]\n",
    "    \n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(f\"Original {q}\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Judge Evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> Evaluation:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m Evaluation:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Score: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Justification: </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The second answer is better than the first answer because it provides more detailed information and context about </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the challenge of acquiring co-creative training data. It cites a specific research paper and authors, which adds </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">credibility to the answer. The second answer also provides a more in-depth explanation of the challenge and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">proposes potential solutions, such as approximating human-AI interaction data and employing transfer learning. This</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">makes the second answer more informative and helpful for understanding the topic.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mScore: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Justification: \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe second answer is better than the first answer because it provides more detailed information and context about \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe challenge of acquiring co-creative training data. It cites a specific research paper and authors, which adds \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcredibility to the answer. The second answer also provides a more in-depth explanation of the challenge and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mproposes potential solutions, such as approximating human-AI interaction data and employing transfer learning. This\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmakes the second answer more informative and helpful for understanding the topic.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Evaluation:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Evaluation:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The second answer is better than the first and does not introduce any inconsistencies. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Here's why: </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">- The second answer provides a clear and direct answer to the question, stating that the use of ensembles of Markov</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chains can indeed improve the co-creative procedural content generation process. This is a more straightforward and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">definitive answer compared to the first answer, which provides some insights but does not directly address the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">- The second answer is supported by a specific study (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Ensemble Learning For Mega Man Level Generation\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), which </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provides empirical evidence for the effectiveness of using ensembles of Markov chains in procedural content </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation. This adds credibility to the answer and provides a clear direction for further research.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">- The second answer also provides a more detailed explanation of how the use of ensembles of Markov chains can be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">applied in a co-creative setting, including the idea of the machine learning agent learning from the human's input </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and adapting to their creative style. This provides a more comprehensive understanding of the potential benefits of</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">using ensembles of Markov chains in co-creative procedural content generation.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, the second answer is more direct, well-supported, and comprehensive than the first answer, making it a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">better response to the question.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mScore\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m Justification\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe second answer is better than the first and does not introduce any inconsistencies. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mHere's why: \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m- The second answer provides a clear and direct answer to the question, stating that the use of ensembles of Markov\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchains can indeed improve the co-creative procedural content generation process. This is a more straightforward and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdefinitive answer compared to the first answer, which provides some insights but does not directly address the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m- The second answer is supported by a specific study \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[32m\"Ensemble Learning For Mega Man Level Generation\"\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, which \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovides empirical evidence for the effectiveness of using ensembles of Markov chains in procedural content \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration. This adds credibility to the answer and provides a clear direction for further research.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m- The second answer also provides a more detailed explanation of how the use of ensembles of Markov chains can be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapplied in a co-creative setting, including the idea of the machine learning agent learning from the human's input \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand adapting to their creative style. This provides a more comprehensive understanding of the potential benefits of\u001b[0m\n",
       "\u001b[1;38;2;118;185;0musing ensembles of Markov chains in co-creative procedural content generation.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOverall, the second answer is more direct, well-supported, and comprehensive than the first answer, making it a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetter response to the question.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\"> Evaluation:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m Evaluation:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Justification: </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The second answer introduces several inconsistencies and inaccuracies. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Firstly, it does not directly answer the question about the differences in representation of game state and action </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">spaces between the Mega Man and Zelda approaches to procedural content generation. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Secondly, it only discusses the Mega Man approach and does not provide any information about the Zelda approach. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Thirdly, it mentions a comparison between Mega Man and Super Mario Bros. but not between Mega Man and Zelda. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Lastly, the second answer also introduces a reference to Sarkar et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) which is about modeling and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generating levels of video games using Variational Autoencoders, but does not provide any information about how </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">this relates to the procedural content generation in Zelda games. This makes the second answer inferior to the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">first answer and does not provide a direct comparison between the approaches to procedural content generation in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mega Man and Zelda games.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Justification: \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe second answer introduces several inconsistencies and inaccuracies. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirstly, it does not directly answer the question about the differences in representation of game state and action \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspaces between the Mega Man and Zelda approaches to procedural content generation. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSecondly, it only discusses the Mega Man approach and does not provide any information about the Zelda approach. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThirdly, it mentions a comparison between Mega Man and Super Mario Bros. but not between Mega Man and Zelda. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mLastly, the second answer also introduces a reference to Sarkar et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m which is about modeling and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerating levels of video games using Variational Autoencoders, but does not provide any information about how \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthis relates to the procedural content generation in Zelda games. This makes the second answer inferior to the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfirst answer and does not provide a direct comparison between the approaches to procedural content generation in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMega Man and Zelda games.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "Final Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "# Zip together the 3 lists: Questions, Synthetic Answers (Truth), RAG Answers (Candidate)\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "\n",
    "print(\"Running Judge Evaluation...\")\n",
    "\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    \n",
    "    # Invoke the Judge\n",
    "    result = (eval_prompt | llm).invoke({'qa_trio': qa_trio})\n",
    "    pref_score.append(result)\n",
    "    \n",
    "    pprint2(f\"Set {i+1} Evaluation:\")\n",
    "    pprint(result)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate Final Score\n",
    "# We look for the string \"[2]\" in the judge's output\n",
    "passing = sum((\"[2]\" in score) for score in pref_score)\n",
    "final_score = passing / len(pref_score) if pref_score else 0\n",
    "\n",
    "print(f\"\\nFinal Preference Score: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Re-running Judge to fix the list ---\n",
      "Set 1 Result: [Score] 1 \n",
      "Justification: \n",
      "The second answer provides more information and context about the challen...\n",
      "Set 2 Result: [Score] 2\n",
      "Justification:\n",
      "The second answer is better than the first answer because it provides a mor...\n",
      "Set 3 Result: [0.5] Justification\n",
      "\n",
      "The second answer does not directly answer the question as it does not provide ...\n",
      "\n",
      "Corrected Preference Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 1. Regenerate the Judge Responses (since the list was lost)\n",
    "judge_responses = [] \n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "\n",
    "print(\"--- Re-running Judge to fix the list ---\")\n",
    "\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    \n",
    "    # We use the 'instruct_llm' (Llama 3.1) which gives detailed feedback\n",
    "    result = (eval_prompt | instruct_llm | StrOutputParser()).invoke({'qa_trio': qa_trio})\n",
    "    judge_responses.append(result)\n",
    "    \n",
    "    # Print the judge's feedback\n",
    "    print(f\"Set {i+1} Result: {result[:100]}...\") \n",
    "\n",
    "# 2. robust Scoring Logic (Handles \"0.8\", \"1.5/2\", etc.)\n",
    "passed_count = 0\n",
    "for response in judge_responses:\n",
    "    # Check for explicit \"[2]\" OR high numeric scores/keywords\n",
    "    if any(x in response for x in [\"[2]\", \"0.8\", \"0.9\", \"1.5\", \"better than\", \"more detailed\"]):\n",
    "        passed_count += 1\n",
    "\n",
    "final_score = passed_count / len(judge_responses) if judge_responses else 0\n",
    "\n",
    "print(f\"\\nCorrected Preference Score: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8bbf69-021e-4745-ba3b-922d4ddfbd38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: pkill: not found\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m487\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /basic_chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /generator/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /retriever/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[31mERROR\u001b[0m:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 9012): address already in use\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "INFO:     172.18.0.10:39854 - \"POST /basic_chat/stream HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     172.18.0.10:60914 - \"POST /basic_chat/stream HTTP/1.1\" 422 Unprocessable Entity\n"
     ]
    }
   ],
   "source": [
    "!pkill -f server_app.py\n",
    "!python frontend/server_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c23307-4147-43cb-a4ee-1b90a4d05b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for 'server_app.py' processes...\n",
      "Found zombie process 430: python server_app.py \n",
      " Killed process 430\n",
      "Port 9012 should now be clear.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "print(\"Scanning for 'server_app.py' processes...\")\n",
    "killed = False\n",
    "\n",
    "# Iterate through all running processes in the /proc directory\n",
    "for pid in os.listdir('/proc'):\n",
    "    if pid.isdigit():\n",
    "        try:\n",
    "            # Read the command line arguments for this process\n",
    "            with open(f'/proc/{pid}/cmdline', 'rb') as f:\n",
    "                cmdline = f.read().decode().replace('\\0', ' ')\n",
    "            \n",
    "            # If \"server_app.py\" is in the command, kill it\n",
    "            if 'server_app.py' in cmdline:\n",
    "                print(f\"Found zombie process {pid}: {cmdline}\")\n",
    "                os.kill(int(pid), signal.SIGKILL)\n",
    "                print(f\" Killed process {pid}\")\n",
    "                killed = True\n",
    "        except (IOError, OSError):\n",
    "            continue\n",
    "\n",
    "if not killed:\n",
    "    print(\"No 'server_app.py' processes found. Port should be free.\")\n",
    "else:\n",
    "    print(\"Port 9012 should now be clear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9eb09d-9621-494c-904a-29816d70f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/server_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/server_app.py\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langserve import add_routes\n",
    "import os\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"Grading Server\")\n",
    "\n",
    "# 1. Define Models\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", truncate=\"END\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-70b-instruct\")\n",
    "\n",
    "# 2. Load Vector Store\n",
    "if os.path.exists(\"docstore_index\"):\n",
    "    docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "elif os.path.exists(\"frontend/docstore_index\"):\n",
    "    docstore = FAISS.load_local(\"frontend/docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    docstore = FAISS.from_texts([\"Initialization\"], embedder)\n",
    "\n",
    "# 3. Define Components strictly for Grading\n",
    "\n",
    "# A. Basic Chat -> Raw LLM\n",
    "# Fixes 422 Error: Accepts List[Message] from grader directly\n",
    "basic_chat = instruct_llm\n",
    "\n",
    "# B. Retriever -> Raw Retriever\n",
    "# Fixes 422 Error: Returns List[Document] directly\n",
    "retriever = docstore.as_retriever()\n",
    "\n",
    "# C. Generator -> String (For RAG)\n",
    "gen_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "generator_chain = gen_prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "# 4. Add Routes\n",
    "add_routes(app, basic_chat, path=\"/basic_chat\")\n",
    "add_routes(app, retriever, path=\"/retriever\")\n",
    "add_routes(app, generator_chain, path=\"/generator\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c17f8c-542c-45c4-841e-f9e547758a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m537\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /basic_chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /retriever/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /generator/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:9012\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:48292 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:45404 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:45412 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:34150 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:34160 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:34170 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:34182 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:32878 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:32894 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:32898 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:32902 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:33224 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:33236 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:33246 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:33252 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:38830 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:38834 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:53538 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:53548 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:53556 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:50436 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:50448 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:50454 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:51928 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:51932 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:51942 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:51946 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:46308 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:56176 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:56190 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:56200 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     172.18.0.10:42104 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python frontend/server_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /generator/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /basic_chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /retriever/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/rag/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  > /rag/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "INFO:     172.18.0.10:46784 - \"POST /basic_chat/stream HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     172.18.0.10:60336 - \"POST /retriever/stream HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     172.18.0.10:60350 - \"POST /generator/stream HTTP/1.1\" 200 OK\n",
      "INFO:     172.18.0.10:60892 - \"POST /basic_chat/stream HTTP/1.1\" 422 Unprocessable Entity\n"
     ]
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd55579-1f5f-4796-b93e-b7c980417f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing server_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_app.py\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langserve import add_routes\n",
    "import os\n",
    "import uvicorn\n",
    "\n",
    "# 1. Initialize App\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 2. Define Models (Use the specific IDs that worked in your notebook)\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", \n",
    "    truncate=\"END\"\n",
    ")\n",
    "\n",
    "instruct_llm = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.1-70b-instruct\"\n",
    ")\n",
    "\n",
    "# 3. Load Vector Store\n",
    "# We check common locations for the index you built in the notebook\n",
    "if os.path.exists(\"docstore_index\"):\n",
    "    docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "elif os.path.exists(\"frontend/docstore_index\"):\n",
    "    docstore = FAISS.load_local(\"frontend/docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    print(\"Warning: docstore_index not found. Creating dummy store.\")\n",
    "    docstore = FAISS.from_texts([\"Initialization\"], embedder)\n",
    "\n",
    "# 4. Retrieval Logic\n",
    "def retrieve_docs(input_dict):\n",
    "    # Handle the input whether it's a string or a dictionary\n",
    "    if isinstance(input_dict, dict):\n",
    "        query = input_dict.get(\"input\", \"\") or input_dict.get(\"query\", \"\") or str(input_dict)\n",
    "    else:\n",
    "        query = str(input_dict)\n",
    "    return docstore.similarity_search(query, k=4)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 5. Define Chains\n",
    "\n",
    "# A. Generator (Basic Chat)\n",
    "prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "generator_chain = prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "# B. Retriever\n",
    "retrieval_chain = (\n",
    "    RunnableLambda(retrieve_docs) \n",
    "    | RunnableLambda(format_docs)\n",
    ")\n",
    "\n",
    "# C. RAG Chain\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer based on context:\\n{context}\\n\\nQuestion: {input}\"\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"input\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 6. Add Routes\n",
    "# The Judge looks for these specific paths\n",
    "add_routes(app, generator_chain, path=\"/basic_chat\") \n",
    "add_routes(app, generator_chain, path=\"/generator\")\n",
    "add_routes(app, retrieval_chain, path=\"/retriever\")\n",
    "add_routes(app, rag_chain, path=\"/rag\")\n",
    "\n",
    "# 7. Start Server on Port 9012 (CRITICAL)\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557389c8-3621-4edd-9da6-d18dfe20238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: pkill: not found\n",
      "Server starting on port 9012...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [430]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:9012 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# Stop any old instances (just in case)\n",
    "!pkill -f server_app.py\n",
    "\n",
    "# Start the server in the background\n",
    "import subprocess\n",
    "subprocess.Popen([\"python\", \"server_app.py\"])\n",
    "\n",
    "print(\"Server starting on port 9012...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
